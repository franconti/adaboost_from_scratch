{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor. With each iteration, the weak rules from each individual classifier are combined to form one, strong prediction rule.\n",
        "\n",
        "If the weak learners are trained in parallel, it is called Baaging. Bagging and boosting are two main types of ensemble learning methods.\n",
        "AdaBoost, short for Adaptive Boosting, represents a prominent boosting algorithm widely used for classification tasks.\n",
        "\n",
        "\n",
        "\n",
        "The steps in adaboost are:\n",
        "\n",
        "**1- Initiate the observation weights:**\n",
        " Begin by assigning equal weights, such that w_i = 1/N , i = 1,2,3... N\n",
        "\n",
        " At the beguining all the points have equal weights. Note that the weighted samples always sum to 1, so the value of each individual weight will always lie between 0 and 1.\n",
        "\n",
        "\n",
        "**2- Iteration:**\n",
        "\n",
        "\n",
        " **2.1- Fitting the Classifier:** Train a classifier using the current weights w_i\n",
        "\n",
        " **2.2- Computing the Error: ** Calculate the error rate ε as the weighted sum of incorrectly classified samples divided by the sum of observation weights:\n",
        " ε = Σ(w_i * Incorrectly classified) / Σ(w_i)\n",
        "\n",
        " **2.3- Determining Significance (α):**  It is the importance (influence) of this model in the final clasification. Significance is inversely proportional to error.\n",
        " α = 0.5 * ln((1 - ε) / ε)\n",
        "\n",
        "\n",
        " **2.4- Updating Weights:** Adjust weights by reducing those associated with correctly classified training samples. Misclassified points receive increased weight, enhancing the algorithm's focus on correctly classifying them in next round.\n",
        "\n",
        "\n",
        "**3- Generatig output:**\n",
        " The final prediction is obtained by summing all the predicted values multiplied by their corresponding model significances:\n",
        "Output G(x) = sign [∑ alpha * G(x)]\n",
        "\n",
        "\n",
        "This is how the combination Error, Alpha and Weights is used to create iteratively better learners.\n",
        "\n",
        "\n",
        "The goal of this project is to build AdaBoost implementation in Python from scratch using Decision Trees, SVMs (Support Vector Machines), and Logistic Regression as base learners. We will then compare its performance, such as accuracy against both standalone Decision Trees and the AdaBoost library available in scikit-learn.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### References\n",
        "https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning\n",
        "\n",
        "https://www.ibm.com/topics/boosting\n",
        "\n",
        "https://statchaitya.github.io/adaboostclassifier/\n",
        "\n",
        "https://blog.paperspace.com/adaboost-optimizer/#:~:text=Alpha%20is%20how%20much%20influence,ranging%20from%200%20to%201.\n",
        "\n",
        "\n",
        "https://xavierbourretsicotte.github.io/AdaBoost.html"
      ],
      "metadata": {
        "id": "m7Nf4EBlgDBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data set\n",
        "\n",
        "This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.\n",
        "\n",
        "Columns\n",
        "1- age\n",
        "\n",
        "2- sex\n",
        "\n",
        "3- chest pain type (4 values)\n",
        "\n",
        "4- resting blood pressure\n",
        "\n",
        "5- serum cholestoral in mg/dl\n",
        "\n",
        "6- fasting blood sugar > 120 mg/dl\n",
        "\n",
        "7- resting electrocardiographic results (values 0,1,2)\n",
        "\n",
        "8- maximum heart rate achieved\n",
        "\n",
        "9- exercise induced angina\n",
        "\n",
        "10- oldpeak = ST depression induced by exercise relative to rest\n",
        "\n",
        "11- the slope of the peak exercise ST segment\n",
        "\n",
        "12- number of major vessels (0-3) colored by flourosopy\n",
        "\n",
        "13- thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n",
        "\n",
        "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\n",
        "\n",
        "\n",
        "### Reference\n",
        "https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset"
      ],
      "metadata": {
        "id": "aRTn-DknCrik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "gmIwoGU5l6bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "eJcXC1cMmq2K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZIsClCq8n_U",
        "outputId": "19503869-0c03-446b-f14d-21001c2b993b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the path to your CSV file\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/heart.csv'\n",
        "\n",
        "# Read CSV file into a DataFrame\n",
        "data = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "kBtI3YDk8wKP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu6f3Fr19DGh",
        "outputId": "0bb885ce-482c-47b1-ae7e-8a3a5bff9787"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
              "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CmrksYlefnIi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/heart.csv'\n",
        "\n",
        "class Adaboost:\n",
        "\n",
        "    def __init__(self,file_name):\n",
        "        self.file_name = file_name\n",
        "\n",
        "    def read_and_organize_data(self):\n",
        "\n",
        "        full_path = file_path\n",
        "        dt = pd.read_csv(full_path)\n",
        "\n",
        "        #conversion to handle categorical variables properly before further processing.\n",
        "        #here are 4 ways to achive the same result\n",
        "        dt['cp'] = dt['cp'].astype('category')\n",
        "        dt['fbs'] = dt['fbs'].astype(str)\n",
        "        dt['restecg'] = dt['restecg'].apply(str)\n",
        "        dt['exang'] = dt['exang'].map(str)\n",
        "        dt['slope'] = dt['slope'].astype(str)\n",
        "        dt['thal'] = dt['thal'].astype(str)\n",
        "\n",
        "        #get_dummiesconverts categorical variables into dummy/indicator variables.\n",
        "        #it creates a new DataFrame with binary columns for each category in the specified column(s).\n",
        "        dt = pd.get_dummies(dt, drop_first=True)\n",
        "\n",
        "\n",
        "        #Extracts target value and convert it to  -1 and 1\n",
        "        #This is a case of Binary Classification. Using -1 and +1 facilitates the calculation of errors, where misclassified samples can easily be identified by their signs\n",
        "        y_vals = dt['target'].values\n",
        "        y_vals = 2*y_vals-1\n",
        "        data = dt.drop('target',axis=1).values\n",
        "\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = \\\n",
        "            train_test_split(data,y_vals , test_size = .25)\n",
        "        return self.x_train, self.x_test, self.y_train, self.y_test\n",
        "\n",
        "\n",
        "    def fit(self,num_estimators=100,base_model = DecisionTreeClassifier,**params):\n",
        "\n",
        "\n",
        "        # self._read_and_organize_data()\n",
        "\n",
        "        num_samples = len(x_train)\n",
        "\n",
        "        self.estimator_list, self.estimator_weight_list = [], []\n",
        "\n",
        "        #initializes a weight vector with equal weight for each sample in the training set.\n",
        "        sample_weight = np.ones(num_samples) / num_samples\n",
        "\n",
        "        for i in range(num_estimators):\n",
        "\n",
        "            estimator = base_model(**params)\n",
        "\n",
        "            estimator.fit(self.x_train, self.y_train, sample_weight=sample_weight)\n",
        "            y_predict = estimator.predict(self.x_train)\n",
        "\n",
        "            #calculationg error, misclassification\n",
        "            incorrect = (y_predict != self.y_train)\n",
        "\n",
        "            #total error\n",
        "            estimator_error =  np.average(incorrect, weights=sample_weight, axis=0)\n",
        "\n",
        "            #significance\n",
        "            significance =  np.log((1. - estimator_error) / estimator_error)\n",
        "\n",
        "            #update weights\n",
        "\n",
        "            #updates the sample weights based on the incorrect boolean array by exponentiating the significance term where the predictions were incorrect.\n",
        "            sample_weight *= np.exp(significance * incorrect)\n",
        "            #Normalizes the sample weights\n",
        "            sample_weight /= sample_weight.sum()\n",
        "\n",
        "            #Appends the current estimator and its significance\n",
        "            self.estimator_list.append(estimator)\n",
        "            self.estimator_weight_list.append(significance)\n",
        "\n",
        "\n",
        "        #Reshapes the list of estimator weights into a 2D array.\n",
        "        self.estimator_weight_list = np.array(self.estimator_weight_list).reshape(-1,1)\n",
        "\n",
        "    def predict(self):\n",
        "\n",
        "        y_test_pred_list = [model.predict(self.x_test) for model in self.estimator_list]\n",
        "\n",
        "        #organizing arrays for matrix multiplication\n",
        "        y_test_pred_list = np.asarray(y_test_pred_list)\n",
        "\n",
        "        #Transposes y_test_pred_list to align the dimensions for matrix multiplication.\n",
        "        preds = np.sign(y_test_pred_list.T@self.estimator_weight_list) #np.sign() applies the sign function to the result of the matrix multiplication, converting the resulting values to +1 or -1.\n",
        "\n",
        "        #calculates accuracy\n",
        "        accuracy = accuracy_score(preds,self.y_test)\n",
        "        return preds,accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Xf9-se0gXKu2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running predictions for all different the models"
      ],
      "metadata": {
        "id": "XXdsreohV91E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    file_name = 'heart.xls'\n",
        "    adaboost_obj = Adaboost(file_name)\n",
        "    x_train, x_test, y_train, y_test = adaboost_obj.read_and_organize_data()\n",
        "\n",
        "    my_dct = DecisionTreeClassifier(max_depth=6)\n",
        "    my_dct.fit(x_train,y_train)\n",
        "    dct_score = my_dct.score(x_test,y_test)\n",
        "    print('Decision tree accuracy: ',dct_score)\n",
        "\n",
        "    my_adaboost = AdaBoostClassifier()\n",
        "    my_adaboost.fit(x_train,y_train)\n",
        "    adaboost_score = my_adaboost.score(x_test,y_test)\n",
        "    print('Scikit learn adaboost accuracy: ',adaboost_score)\n",
        "\n",
        "    adaboost_obj.fit(50,DecisionTreeClassifier,max_depth=1)\n",
        "    preds,adaboost_score = adaboost_obj.predict()\n",
        "    print('Adaboost accuracy DecisionTree: ',adaboost_score)\n",
        "\n",
        "    adaboost_obj.fit(100,SVC,kernel='linear')\n",
        "    preds,adaboost_score_with_svm = adaboost_obj.predict()\n",
        "    print('Adaboost accuracy with SVC: ',adaboost_score_with_svm)\n",
        "\n",
        "    adaboost_obj.fit(100,LogisticRegression,C=1000)\n",
        "    preds,adaboost_score_with_logistic = adaboost_obj.predict()\n",
        "    print('Adaboost accuracy with logistic: ',adaboost_score_with_logistic)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRsdSG6SMV_g",
        "outputId": "e41f0934-1882-4a15-9047-14113ac4de59"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree accuracy:  0.6447368421052632\n",
            "Scikit learn adaboost accuracy:  0.7236842105263158\n",
            "Adaboost accuracy DecisionTree:  0.8026315789473685\n",
            "Adaboost accuracy with SVC:  0.7894736842105263\n",
            "Adaboost accuracy with logistic:  0.8026315789473685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to just using Decision Trees, AdaBoost gives us a bigger accuracy. Moreover, the algorithm developed from scratch achieves a similar level of accuracy as the one obtained using the scikit-learn library."
      ],
      "metadata": {
        "id": "MwjCYIJhWY82"
      }
    }
  ]
}